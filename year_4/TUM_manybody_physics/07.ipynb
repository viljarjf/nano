{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "default_dtype = np.complex128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from 05/lanczos.py\n",
    "Id = sp.csr_matrix(np.eye(2), dtype=default_dtype)\n",
    "Sx = sp.csr_matrix([[0., 1.], [1., 0.]], dtype=default_dtype)\n",
    "Sz = sp.csr_matrix([[1., 0.], [0., -1.]], dtype=default_dtype)\n",
    "Splus = sp.csr_matrix([[0., 1.], [0., 0.]], dtype=default_dtype)\n",
    "Sminus = sp.csr_matrix([[0., 0.], [1., 0.]], dtype=default_dtype)\n",
    "\n",
    "\n",
    "def singlesite_to_full(op, i, L):\n",
    "    op_list = [Id]*L  # = [Id, Id, Id ...] with L entries\n",
    "    op_list[i] = op\n",
    "    full = op_list[0]\n",
    "    for op_i in op_list[1:]:\n",
    "        full = sp.kron(full, op_i, format=\"csr\")\n",
    "    return full\n",
    "\n",
    "\n",
    "def gen_sx_list(L):\n",
    "    return [singlesite_to_full(Sx, i, L) for i in range(L)]\n",
    "\n",
    "\n",
    "def gen_sz_list(L):\n",
    "    return [singlesite_to_full(Sz, i, L) for i in range(L)]\n",
    "\n",
    "\n",
    "def _gen_hamiltonian(sx_list, sz_list, g, J=1.):\n",
    "    L = len(sx_list)\n",
    "    H = sp.csr_matrix((2**L, 2**L), dtype=default_dtype)\n",
    "    for j in range(L):\n",
    "        H += - g * sz_list[j]\n",
    "        # open boundary\n",
    "        if j:\n",
    "            H += - J *( sx_list[j] * sx_list[(j-1)])\n",
    "    return H\n",
    "\n",
    "def gen_hamiltonian(L: int, g: float, J: float = 1.0) -> sp.csr_matrix:\n",
    "    return _gen_hamiltonian(gen_sx_list(L), gen_sz_list(L), g, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 14\n",
    "J = 1\n",
    "g = 1.5\n",
    "H = gen_hamiltonian(L=L, g=g, J=J)\n",
    "(E0,), psi_0 = sp.linalg.eigsh(H, k=1, which=\"SA\")\n",
    "psi_0 = psi_0.reshape(-1, 1)\n",
    "\n",
    "# It's already normalized\n",
    "assert np.allclose(np.dot(np.conj(psi_0).T, psi_0), 1)\n",
    "\n",
    "print(f\"{E0 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(psi: np.ndarray, L: int, chi_max: int) -> list[np.ndarray]:\n",
    "    \"\"\"returns list of L 3D ndarrays\"\"\"\n",
    "    res = []\n",
    "    psi_n = psi.reshape(1, -1)\n",
    "    for n in range(L):\n",
    "        chi_n, dim_R_n = psi_n.shape\n",
    "        psi_n = psi_n.reshape(2*chi_n, dim_R_n//2)\n",
    "\n",
    "        M_n, lambda_n, psi_n_tilde = np.linalg.svd(psi_n, full_matrices=False)\n",
    "                \n",
    "        if lambda_n.size > chi_max:\n",
    "            # Stolen from the exercise sheet\n",
    "            keep = np.argsort(lambda_n)[:: -1][: chi_max ]  # indices to keep\n",
    "            M_n = M_n[: , keep]                             # truncate matrix\n",
    "            lambda_n = lambda_n[ keep ]                     # truncate lambdas\n",
    "            psi_n_tilde = psi_n_tilde[ keep , :]            # truncate psi_tilde\n",
    "        psi_n = lambda_n[:, np.newaxis] * psi_n_tilde[:, :]\n",
    "        # End of stolen code\n",
    "        \n",
    "        chi_np1 = lambda_n.size\n",
    "\n",
    "        res.append(M_n.reshape(chi_n, 2, chi_np1))\n",
    "    return res\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def compress_full_tensor(psi: np.ndarray, L: int, chi_max: int) -> np.ndarray:\n",
    "    return reduce(\n",
    "        lambda a, b: np.tensordot(a, b, axes=(-1, 0)), \n",
    "        compress(psi, L, chi_max),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can recreate the original state from MPS\n",
    "M = compress_full_tensor(psi_0, L, 2**(L//2))\n",
    "print(f\"{M.shape = }\")\n",
    "assert np.allclose(M.ravel(), psi_0.ravel())\n",
    "del M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find maximum compression ratio\n",
    "# \"Lossless\" as defined by np.allclose\n",
    "# Man, do while would be nice right now\n",
    "def find_compression_ratio(psi: np.ndarray, L: int, chi_0: int = 20) -> None:\n",
    "    M = compress_full_tensor(psi, L, 2**(L//2))\n",
    "    chi = chi_0\n",
    "    while np.allclose(M.ravel(), psi.ravel()):\n",
    "        chi -= 1\n",
    "        M = compress_full_tensor(psi, L, chi)\n",
    "    \n",
    "    if chi == chi_0:\n",
    "        print(\"MPS representation did not correspond to original representation \"\n",
    "              f\"with {chi_0 = }.\")\n",
    "        return\n",
    "\n",
    "    # Found the first chi where we have compression losses\n",
    "    chi += 1\n",
    "    print(f\"{chi = }\")\n",
    "    Ms_full_size = sum([M.size for M in compress(psi, L, 2**(L//2))])\n",
    "    Ms_comp_size = sum([M.size for M in compress(psi, L, chi)])\n",
    "\n",
    "    compression_ratio = Ms_full_size / Ms_comp_size\n",
    "    print(f\"{compression_ratio = :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ground state: \")\n",
    "find_compression_ratio(psi_0, L, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random state: \")\n",
    "psi_rnd = np.random.normal(size=(2**L)) + 1j * np.random.normal(size=(2**L))\n",
    "psi_rnd /= np.dot(np.conj(psi_rnd).T, psi_rnd)\n",
    "find_compression_ratio(psi_rnd, L, 2**(L//2))\n",
    "del psi_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(MPS_a: list[np.ndarray], MPS_b: list[np.ndarray]) -> float:\n",
    "    # Sum over alpha_0 and j_0\n",
    "    res = np.tensordot(MPS_a[0], MPS_b[0].conj(), axes=((0, 1), (0, 1)))\n",
    "\n",
    "    for Ma, Mb in zip(MPS_a[1:], MPS_b[1:]):\n",
    "        # Sum over j_i\n",
    "        T = np.tensordot(Ma, Mb.conj(), axes=(1, 1))\n",
    "        # Sum over alpha_j both above and below\n",
    "        res = np.tensordot(res, T, axes=((0, 1), (0, 2)))\n",
    "    return res.item().real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_exact = compress(psi_0, L, 2**(L//2) + 1)\n",
    "psi_compr = compress(psi_0, L, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{overlap(psi_exact, psi_exact) = }\")\n",
    "print(f\"{overlap(psi_compr, psi_compr) = }\")\n",
    "print(f\"{overlap(psi_exact, psi_compr) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_up = np.zeros((1, 2, 1))\n",
    "M_up[0, 0, 0] = 1.\n",
    "MPS_all_up = [M_up.copy() for _ in range(L)]\n",
    "print(f\"{overlap(psi_exact, MPS_all_up) = }\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
