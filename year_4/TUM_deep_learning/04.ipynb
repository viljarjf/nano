{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e97ec3ef",
   "metadata": {},
   "source": [
    "## Building a Multilayer Perceptron\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5491dda",
   "metadata": {},
   "source": [
    "Let's follow the lecture and build a multilayer perceptron to approximate the function $\\theta(x)$ in our variational inference model $$\\mathrm{Bernoulli}(z|\\theta(x))$$ with the following structure\n",
    "\n",
    "* 2 input nodes\n",
    "* 2 internal features computed using ReLu activation\n",
    "* 1 output feature with sigmoid activation that \n",
    "\n",
    "From Ex3, we had already the following code ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c044c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(X,pars):\n",
    "    out = X @ pars[:2].T + pars[2]\n",
    "    grad_pars = np.column_stack([X,np.ones(len(X))])\n",
    "    return out,grad_pars\n",
    "\n",
    "def sigmoid(x):\n",
    "    out = 1/(1+np.exp(-x))\n",
    "    grad = out*(1-out)\n",
    "    return out,grad.reshape(-1,1)\n",
    "\n",
    "def theta_perceptron(X,pars):\n",
    "    feature1,grad_linear1 = linear(X,pars[0:3])\n",
    "    activation1,grad_sigmoid1 = sigmoid(feature1)\n",
    "    return activation1, grad_sigmoid1*grad_linear1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f0081f5",
   "metadata": {},
   "source": [
    "* Write a function `relu(x)` that produces the relu activation function and its gradient.\n",
    "* make sure the gradient is returned as a shape (N,1)\n",
    "* plot it for the range: `x = np.linspace(-5,5,1001)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb08101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    out = np.where(x < 0, 0, x)\n",
    "    grad = np.where(x < 0, 0, 1)\n",
    "    return out, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(-5,5,1001)\n",
    "r,g = relu(xi)\n",
    "plt.plot(xi,r)\n",
    "plt.plot(xi,g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b75937e7",
   "metadata": {},
   "source": [
    "Write a function `multilayer_perceptron(X,pars)` that takes 9 parameters (3 for each artificial neurons: 2 weights and 1 bias) that can compute the $\\theta(x)$ prediction for many input points at once (batched computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X,pars):\n",
    "    feature1,grad_f1 = linear(X,pars[0:3])\n",
    "    feature2,grad_f2 = linear(X,pars[3:6])\n",
    "    activation1,grad_r1 = relu(feature1)\n",
    "    activation2,grad_r2 = relu(feature2)\n",
    "    hidden = np.column_stack([activation1,activation2])\n",
    "    output_feature,grad_f3 = linear(hidden,pars[6:9])\n",
    "    theta,grad_sig = sigmoid(output_feature)\n",
    "    return theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5035bd6",
   "metadata": {},
   "source": [
    "Use this function to plot the contour of the multilayer perceptron\n",
    "\n",
    "```\n",
    "def plot_contour(func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    X = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "    out = func(X,pars)\n",
    "    out = np.swapaxes(out.reshape(101,101),0,-1)\n",
    "    plt.contour(grid[0],grid[1],out)\n",
    "```\n",
    "\n",
    "Try for example parameter vectors: `np.array([1,0,0,0,1,0,1,1,0])` or what every you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ec581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    X = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "    out = func(X,pars)\n",
    "    out = np.swapaxes(out.reshape(101,101),0,-1)\n",
    "    plt.contour(grid[0],grid[1],out)\n",
    "    \n",
    "plot_contour(multilayer_perceptron,np.array([-.2,1,0.0,.2,1,0.0,-.1,-.1,0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdcddc2a",
   "metadata": {},
   "source": [
    "Now comes the hard part!\n",
    "\n",
    "We want to compute gradients for this function\n",
    "\n",
    "$$\n",
    "\\vec{a} = [\\;\\mathrm{ReLU}(\\mathrm{Lin}(\\vec{x},\\phi_1))\\;,\\;\\mathrm{ReLU}(\\mathrm{Lin}(\\vec{x},\\phi_2))\\;]\\\\\n",
    "a_2 = \\sigma(\\mathrm{Lin}(\\vec{a},\\phi_3))\\\\\n",
    "$$\n",
    "\n",
    "To compute the gradient we need to also have the gradients $\\frac{\\partial \\mathrm{Lin}}{\\partial \\vec{x}}$\n",
    "\n",
    "* Write a new function that also outputs the partial derivatives with respect to $\\vec{x}$\n",
    "* Hint: the output shape of `grad_x` should be (1,3)!\n",
    "\n",
    "```\n",
    "def linear(X,pars):\n",
    "   ...\n",
    "   return out,grad_pars,grad_x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7717b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X,pars):\n",
    "    out = X @ pars[:2].T + pars[2]\n",
    "    grad_pars = np.column_stack([X,np.ones(len(X))])\n",
    "    grad_x = pars\n",
    "    return out,grad_pars, grad_x.reshape(1,-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83996a36",
   "metadata": {},
   "source": [
    "With this in hadn you can now carefully piece back the gradients together.\n",
    "\n",
    "We can start with the function like this\n",
    "\n",
    "```python\n",
    "def multilayer_perceptron(X,pars):\n",
    "    feature1,grad_f1,_ = linear(X,pars[0:3])\n",
    "    feature2,grad_f2,_ = linear(X,pars[3:6])\n",
    "    activation1,grad_r1 = relu(feature1)\n",
    "    activation2,grad_r2 = relu(feature2)\n",
    "    hidden = np.column_stack([activation1,activation2])\n",
    "    output_feature,grad_f3,grad_x = linear(hidden,pars[6:9]) #here is the new gradient!\n",
    "    theta,grad_sig = sigmoid(output_feature)\n",
    "    return theta\n",
    "```\n",
    "\n",
    "* Try to work out what the 9-dimensional gradient vector looks like for $\\nabla_\\phi \\theta(x,\\phi)$\n",
    "* Hint 1: the gradient should have the shape `(N,9)`\n",
    "* Hint 2: The following would be a correct result\n",
    "\n",
    "```\n",
    "Xtest = np.array([[1.0,1.0]])\n",
    "pars = np.array([0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "multilayer_perceptron(Xtest,pars)\n",
    "\n",
    "value: [0.9552123]\n",
    "gradient: [[0.02994724 0.05989447 0.02994724 0.02738033 0.06845082 0.03422541\n",
    "  0.03422541 0.08556353 0.04278176]]\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X,pars):\n",
    "    feature1,grad_f1,_ = linear(X,pars[0:3])\n",
    "    feature2,grad_f2,_ = linear(X,pars[3:6])\n",
    "    activation1,grad_r1 = relu(feature1)\n",
    "    activation2,grad_r2 = relu(feature2)\n",
    "    hidden = np.column_stack([activation1,activation2])\n",
    "    output_feature,grad_f3,grad_x = linear(hidden,pars[6:9])\n",
    "    theta,grad_sig = sigmoid(output_feature)\n",
    "    \n",
    "    grad1 = grad_sig*grad_x[:,0]*grad_r1*grad_f1\n",
    "    grad2 = grad_sig*grad_x[:,1]*grad_r2*grad_f2\n",
    "    grad3 = grad_sig*grad_f3\n",
    "    grad = np.concatenate([grad1,grad2,grad3],axis=-1)\n",
    "    return theta,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1.0,2.0]])\n",
    "pars = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "value,grad = multilayer_perceptron(Xtest,pars)\n",
    "print(f'value: {value}')\n",
    "print(f'gradient: {grad}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "090e8038",
   "metadata": {},
   "source": [
    "## Training the Perceptron\n",
    "\n",
    "We can adapt the learning code from Exercise 3 to learn the multi-layer perceptron.\n",
    "\n",
    "We can get some interesting datasets from the `scikit-learn` package\n",
    "\n",
    "* Install scikit-learn via `pip install scikit-learn`\n",
    "* Use the following data-generating function and plot the data\n",
    "\n",
    "```\n",
    "def generate_data(N):\n",
    "    import sklearn.datasets as skld\n",
    "    return skld.make_moons(N, noise = 0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    import sklearn.datasets as skld\n",
    "    X,z = skld.make_circles(N, noise = 0.1, factor=0.5)\n",
    "    filt =  (X[:,1] > 0)\n",
    "    return X[filt],z[filt]\n",
    "\n",
    "X,z = generate_data(2000)\n",
    "plt.scatter(X[:,0],X[:,1],c = z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9f44bd",
   "metadata": {},
   "source": [
    "We can now adapt our training code from exercise 3 (take some time to go through it, but nothing fundamental changed) and train out multilayer perceptron!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(z,theta):\n",
    "    out = np.where(z==1,-np.log(theta),-np.log(1-theta))\n",
    "    grad = np.where(z==1,-1/theta,-1/(1-theta)*(-1))\n",
    "    return out,grad.reshape(-1,1)\n",
    "\n",
    "def empirical_risk(X,z,theta_func,pars):\n",
    "    theta,grad_theta = theta_func(X,pars)\n",
    "    loss_val,grad_loss = loss(z,theta)\n",
    "    grad1 = grad_loss*grad_theta\n",
    "    grad = np.concatenate([grad1], axis=-1)\n",
    "    return loss_val.mean(axis=0),grad.mean(axis=0),theta\n",
    "\n",
    "def plot(X,z,theta_func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    Xi = np.swapaxes(grid,0,-1).reshape(-1,2)   \n",
    "    _,_,zi = empirical_risk(Xi,np.zeros(len(Xi)),theta_func,pars)\n",
    "    zi = zi.reshape(101,101).T\n",
    "    plt.contour(grid[0],grid[1],zi, levels = np.linspace(0,1,21))\n",
    "    plt.scatter(X[:,0],X[:,1],c = z)\n",
    "    plt.xlim(-2,2)\n",
    "    plt.ylim(-2,2)\n",
    "\n",
    "def learn(data,pars,theta_func, nsteps = 5000):\n",
    "    X,z = data\n",
    "    for i in range(nsteps):\n",
    "        val,grad,_ = empirical_risk(X,z,theta_func,pars)\n",
    "        pars = pars - 0.01*grad\n",
    "        if i % (nsteps//4) == 0:\n",
    "            print(val,pars)\n",
    "            plot(X,z,theta_func,pars)\n",
    "            plt.gcf().set_size_inches(3,3)\n",
    "            plt.show()\n",
    "    return pars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "912be530",
   "metadata": {},
   "source": [
    "# Try Learning\n",
    "\n",
    "Try learning on a dataset of 1000 samples and initialize the parameters with \n",
    "\n",
    "`pars = np.array([0,1,0,0,1,0,.1,.1,0])`\n",
    "\n",
    "* Note: the point to see here is that non-linear decision boundaries are learned. \n",
    "* The learning itself, depending on the initialization might or might not be super-convincing\n",
    "* Try executing this multiple times to try out different initializations\n",
    "* Ultimately, we will need to add more & more neurons, but as you see the gradient calculation is painful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(generate_data(1000),np.array([-.1,1,0,.1,1,0,-.1,-.1,0]) ,multilayer_perceptron)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "914ef6c6",
   "metadata": {},
   "source": [
    "# Going to many Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODES = 10\n",
    "def linear(X,pars):\n",
    "    out = X @ pars[:-1].T + pars[-1]\n",
    "    grad_pars = np.column_stack([X,np.ones(len(X))])\n",
    "    grad_x = pars[:-1]\n",
    "    return out,grad_pars, grad_x.reshape(1,-1)\n",
    "\n",
    "def many_nodes_mlp(X,pars):\n",
    "    activations = []\n",
    "    grads = []\n",
    "    for i in range(N_NODES):\n",
    "        feature, grad_f,_ = linear(X,pars[3*i:3*(i+1)])\n",
    "        activation,grad_r = relu(feature)\n",
    "        activations.append(activation)\n",
    "        grads.append([grad_r,grad_f])\n",
    "    \n",
    "    hidden = np.column_stack(activations)\n",
    "    output_feature,grad_f3,grad_x = linear(hidden,pars[-(N_NODES+1):])\n",
    "    theta,grad_sig = sigmoid(output_feature)\n",
    "    \n",
    "    grad_components = []\n",
    "    for i in range(N_NODES):\n",
    "        grad = grad_sig*grad_x[:,i]*grads[i][0]*grads[i][1]\n",
    "        grad_components.append(grad)\n",
    "    grad3 = grad_sig*grad_f3\n",
    "    grad = np.concatenate(grad_components+[grad3],axis=-1)\n",
    "    return theta,grad\n",
    "\n",
    "def generate_data(N):\n",
    "    import sklearn.datasets as skld\n",
    "    X,z = skld.make_circles(N, noise = 0.1, factor=0.5)\n",
    "    return X,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(generate_data(1000),np.random.normal(size = (3*N_NODES+N_NODES+1)) ,multilayer_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
