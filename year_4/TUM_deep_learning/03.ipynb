{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d9c882",
   "metadata": {},
   "source": [
    "# Train a Perceptron!\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ok, finally we'll actually train a machine learning system!!\n",
    "\n",
    "The goal is to separata data like this\n",
    "\n",
    "```python\n",
    "def generate_data(N):\n",
    "    s1 = np.random.multivariate_normal([1,1],[[1,0],[0,1]], size = N)\n",
    "    s2 = np.random.multivariate_normal([-1.5,-1.5],[[1,0.2],[0.2,1.0]], size = N)\n",
    "    X = np.concatenate([s1,s2])\n",
    "    z = np.concatenate([np.ones(N),np.zeros(N)])\n",
    "    return X,z\n",
    "\n",
    "X,z = generate_data(200)\n",
    "plt.scatter(X[:,0],X[:,1],c = z)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Setting up Variational Inference\n",
    "\n",
    "We assume the data is produced from a conditional model $p(x|z)$ and we want to find the posterior $p(z|x)$. As an approximation we use the parametrized density \n",
    "\n",
    "$$\n",
    "q(z|\\theta) = \\mathrm{Bernoulli(z|\\theta}) = \\begin{cases}\\theta\\;\\mathrm{for}\\;z = 1\\\\1-\\theta\\;\\mathrm{for}\\;z = 0\\end{cases}\n",
    "$$\n",
    "\n",
    "Following the \"amortized variational inference\" approach, we assume that we can approximate $p(z|x)$ by learning a data-dependent function $\\theta(x)$.\n",
    "\n",
    "\n",
    "The plan is to train a \"soft\" percepton as discussed in the lecture\n",
    "\n",
    "$$\n",
    "\\theta_{\\vec{w},b}(x) = \\sigma(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "such that $p(z|x) \\approx q(z|x) = \\mathrm{Bernoulli(z|\\theta(x)})$\n",
    "\n",
    "We will break this down into three function and combine all parameters into a single parameter vector\n",
    "\n",
    "$$\n",
    "\\phi = (w_1,w_2,b)\n",
    "$$\n",
    "\n",
    "1. First the linear mapping: $h(x) = \\vec{w} x + b$ mapping $h: \\mathbb{R}^2 \\to \\mathbb{R}$\n",
    "   ```python\n",
    "   def linear(x,parameters):\n",
    "       out = x @ parameters[:2].T + parameters[2]\n",
    "       return out\n",
    "   ```\n",
    "\n",
    "2. Second, the squashing function $\\sigma: \\mathbb{R} \\to [0,1]$\n",
    "   ```python\n",
    "   def sigmoid(x):\n",
    "       out = 1/(1+np.exp(-x))\n",
    "       return out\n",
    "   ```\n",
    "\n",
    "2. Third, the loss function $l(z,\\theta) = -\\log q(z_i|\\theta)$\n",
    "   ```python\n",
    "   def loss(z, theta):\n",
    "       out = np.where(z==1,-np.log(theta),-np.log(1-theta))\n",
    "       return out\n",
    "   ```\n",
    "\n",
    "In order to estimate the loss for the full dataset, we need to compute the empirical risk $\\hat{L} = -\\sum_{z_i,x_i} \\log(z_i | \\theta(x_i))$\n",
    "\n",
    "```python\n",
    "def empirical_risk(X,z,pars):\n",
    "    mapped = linear(X,pars)\n",
    "    theta = sigmoid(mapped)\n",
    "    loss_val = loss(z,theta)\n",
    "    return loss_val.mean(axis=0)\n",
    "```\n",
    "\n",
    "We can can plot the decision function $\\theta(x)$ on the plance $(x_1,x_1) \\in \\mathbb{R}^2$ using a function like this\n",
    "\n",
    "```python\n",
    "def plot(X,z,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    Xi = np.swapaxes(grid,0,-1).reshape(-1,2)   \n",
    "    p = sigmoid(linear(X,pars))\n",
    "    zi = sigmoid(linear(Xi,pars))\n",
    "    zi = zi.reshape(101,101).T\n",
    "    plt.contour(grid[0],grid[1],zi)\n",
    "    plt.scatter(X[:,0],X[:,1],c = z)\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(-5,5)\n",
    "```\n",
    "\n",
    "\n",
    "### Step 1 Plot a fixed Percepton\n",
    "\n",
    "Use the above functions to plot the percepton and the data for a fixed parameter vector $\\phi = (2.0,-1.0,0.0)$\n",
    "\n",
    "\n",
    "## Gradient Calculation\n",
    "\n",
    "In order to train the perceptron we need to have the empirical risk value and its derivative\n",
    "\n",
    "$$\\hat{L}, \\frac{\\partial \\hat{L}}{\\partial \\phi}$$\n",
    "\n",
    "Using the definitions we get \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{L}}{\\partial \\phi} = \\sum_i \\frac{\\partial}{\\partial \\phi}l(z_i,\\sigma(h(x))\n",
    "$$\n",
    "\n",
    "\n",
    "That is, if we just need to find an expression for $\\frac{\\partial}{\\partial \\phi}l(z_i,\\sigma(h(x))$. By the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\phi}l(z_i,\\sigma(h(x)) = \\frac{\\partial l}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial h}  \\frac{\\partial h}{\\partial \\phi}\n",
    "$$\n",
    "\n",
    "\n",
    "### Step 2\n",
    "\n",
    "Derive an expression for the the derivative $\\frac{\\partial h}{\\partial \\phi}$\n",
    "\n",
    "\n",
    "### Step 3\n",
    "\n",
    "Show that the derivative of $\\sigma(x)$ corresponds to \n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n",
    "$$\n",
    "\n",
    "### Step 4\n",
    "\n",
    "Derive the derivative of $\\frac{\\partial l(z,\\theta)}{\\partial \\theta}$ for both $z=0$ and $z=1$\n",
    "\n",
    "\n",
    "### Step 5\n",
    "\n",
    "Adapt the functions `linear(..)`, `sigmoid(...)`, `loss(...)` such that they return\n",
    "\n",
    "1. The function value\n",
    "2. The gradient value at the point at which the function was evaluated\n",
    "\n",
    "```\n",
    "def function(...)\n",
    "    out = ...\n",
    "    grad = ...\n",
    "    return out,grad\n",
    "```\n",
    "\n",
    "Hint: as we will evluate the function on many inputs at the same time make sure to return a \"column\" vector of shape (N,1) for the sigmoid and loss functions\n",
    "\n",
    "\n",
    "### Step 6\n",
    "\n",
    "Using the chain rule and the individual gradient computations, adapt `empirical_risk` similarly to return \n",
    "\n",
    "1. $\\hat{L}$\n",
    "2. $\\frac{\\partial \\hat{L}}{\\partial \\phi}$\n",
    "\n",
    "\n",
    "### Step 7\n",
    "\n",
    "Write a Training Loop!\n",
    "\n",
    "1. Initialize the parameters with $\\phi = (1.0,0.0,0.0)$\n",
    "2. Allow 2000 iterations of improving the parameters\n",
    "3. For each iteration update the parameters with Gradient Descent and a Learning Rate of $\\lambda=0.01$\n",
    "\n",
    "$$ \\phi \\leftarrow = \\phi - \\lambda \\frac{\\partial \\hat{L}}{\\partial \\phi}$$\n",
    "\n",
    "4. After every 500 steps, plot the current configuration using `plot(...)` (Note: now that the functions return multiple return values, the `plot` function needs to be adapted a bit\n",
    "\n",
    "### Step 8\n",
    "\n",
    "Congratulations, you wrote your first full ML loop!. You can now play with the details of `generate_data` to try out a few data configurations and convince yourself that your algorithm will find a good decision boundary every time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce420e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff87b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
